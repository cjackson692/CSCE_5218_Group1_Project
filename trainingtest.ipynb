{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58849f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows kept after cleaning: 99996/100000\n",
      "English padded shape: (99996, 72)\n",
      "Tagalog padded shape: (99996, 75)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re, unicodedata\n",
    "import pickle\n",
    "\n",
    "# Pull Data\n",
    "with open(\"clean.en-tl.en\", encoding=\"utf-8\") as f_en:\n",
    "    en_sentences = [line.strip() for line in f_en]\n",
    "\n",
    "with open(\"clean.en-tl.tl\", encoding=\"utf-8\") as f_tl:\n",
    "    tl_sentences = [line.strip() for line in f_tl]\n",
    "\n",
    "# Ensures same number of lines\n",
    "min_len = min(len(en_sentences), len(tl_sentences))\n",
    "min_len = 100000\n",
    "en_sentences = en_sentences[:min_len]\n",
    "tl_sentences = tl_sentences[:min_len]\n",
    "\n",
    "# Make it to where we only accept files w/ same number of lines later\n",
    "\n",
    "# Merge data frames\n",
    "df = pd.DataFrame({\n",
    "    'english': en_sentences,\n",
    "    'tagalog': tl_sentences\n",
    "})\n",
    "\n",
    "# Drop null lines\n",
    "df = df[(df['english'].str.strip() != '') & (df['tagalog'].str.strip() != '')].dropna()\n",
    "\n",
    "def clean(data):\n",
    "    # Normalize unicode and punctutation\n",
    "    data = unicodedata.normalize(\"NFKC\", data)\n",
    "    data = data.replace(\"—\", \"-\").replace(\"–\", \"-\").replace(\"？\", \"?\")\n",
    "\n",
    "    # Remove unnecessary special characters and normalize whitespace\n",
    "    data = re.sub(r\"[\\x00-\\x1F\\x80-\\x9F]\", \" \", data)\n",
    "    data = re.sub(r\"[\\u200b\\u200e\\u202a]\", \" \", data)\n",
    "    data = re.sub(r\"[\\u0370-\\u03FF]\", \" \", data)\n",
    "    data = re.sub(r\"[\\u0900-\\u097F]\", \" \", data)\n",
    "    data = re.sub(r\"[\\u0D80-\\u0DFF]\", \" \", data)\n",
    "    data = re.sub(r\"[\\u0400-\\u04FF]\", \" \", data)\n",
    "    data = re.sub(r\"[\\u4E00-\\u9FFF]\", \" \", data)\n",
    "    data = re.sub(r\"[¢£¤¥¦§©ª®¯°±²³¶¸º¼½¾‰♡♥♪♬✰€]+\", \" \", data)\n",
    "    data = re.sub(r\"<[^>]+>\", \" \", data)\n",
    "    data = re.sub(r\"\\s+\", \" \", data).strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Clean data\n",
    "len1 = len(df)\n",
    "df[\"english\"] = df[\"english\"].apply(clean)\n",
    "df[\"tagalog\"] = df[\"tagalog\"].apply(clean)\n",
    "\n",
    "# After cleaning, remove rows that are empty\n",
    "df = df[(df[\"english\"] != \"\") & (df[\"tagalog\"] != \"\")]\n",
    "len2 = len(df)\n",
    "print(f\"Rows kept after cleaning: {len2}/{len1}\")\n",
    "\n",
    "# Add SOS and EOS\n",
    "df['tagalog'] = df['tagalog'].apply(lambda x: f\"<SOS> {x} <EOS>\")\n",
    "\n",
    "# Tokenize each df\n",
    "en_tokenizer = Tokenizer(filters='', lower=True)\n",
    "tl_tokenizer = Tokenizer(filters='', lower=True)\n",
    "\n",
    "# Converts Word to integer mapping\n",
    "en_tokenizer.fit_on_texts(df['english'])\n",
    "tl_tokenizer.fit_on_texts(df['tagalog'])\n",
    "\n",
    "# Convert Sentence to integers\n",
    "en_sequences = en_tokenizer.texts_to_sequences(df['english'])\n",
    "tl_sequences = tl_tokenizer.texts_to_sequences(df['tagalog'])\n",
    "\n",
    "# Find Max Lengths (used for padding)\n",
    "max_en_len = max(len(seq) for seq in en_sequences)\n",
    "max_tl_len = max(len(seq) for seq in tl_sequences)\n",
    "\n",
    "# Pad Sequence\n",
    "en_padded = pad_sequences(en_sequences, maxlen=max_en_len, padding='post')\n",
    "tl_padded = pad_sequences(tl_sequences, maxlen=max_tl_len, padding='post')\n",
    "\n",
    "en_padded = en_padded\n",
    "tl_padded = tl_padded\n",
    "print(\"English padded shape:\", en_padded.shape)\n",
    "print(\"Tagalog padded shape:\", tl_padded.shape)\n",
    "\n",
    "with open('en_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(en_tokenizer, handle)\n",
    "with open('tl_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tl_tokenizer, handle)\n",
    "with open('en_padded.pickle', 'wb') as handle:\n",
    "    pickle.dump(en_padded, handle)\n",
    "with open('tl_padded.pickle', 'wb') as handle:\n",
    "    pickle.dump(tl_padded, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train shape: (89996, 72)\n",
      "Target Train shape: (89996, 75)\n",
      "Source Test shape: (10000, 72)\n",
      "Target Test shape: (10000, 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 104/704 [04:35<32:45,  3.28s/it]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('en_padded.pickle', 'rb') as f:\n",
    "    en_padded = pickle.load(f)\n",
    "\n",
    "with open('tl_padded.pickle', 'rb') as f:\n",
    "    tl_padded = pickle.load(f)\n",
    "\n",
    "with open('en_tokenizer.pickle', 'rb') as f:\n",
    "    en_tokenizer = pickle.load(f)\n",
    "\n",
    "with open('tl_tokenizer.pickle', 'rb') as f:\n",
    "    tl_tokenizer = pickle.load(f)\n",
    "\n",
    "\n",
    "max_en_len = en_padded.shape[1]\n",
    "max_tl_len = tl_padded.shape[1]\n",
    "\n",
    "source_train, source_test, target_train, target_test = train_test_split(\n",
    "    en_padded,\n",
    "    tl_padded,\n",
    "    test_size=0.1,\n",
    "    random_state=42 \n",
    ")\n",
    "\n",
    "print(f\"Source Train shape: {source_train.shape}\")\n",
    "print(f\"Target Train shape: {target_train.shape}\")\n",
    "print(f\"Source Test shape: {source_test.shape}\")\n",
    "print(f\"Target Test shape: {target_test.shape}\")\n",
    "\n",
    "source_train_tensor = torch.from_numpy(source_train).long()\n",
    "target_train_tensor = torch.from_numpy(target_train).long()\n",
    "source_test_tensor = torch.from_numpy(source_test).long()\n",
    "target_test_tensor = torch.from_numpy(target_test).long()\n",
    "\n",
    "train_dataset = TensorDataset(source_train_tensor, target_train_tensor)\n",
    "test_dataset = TensorDataset(source_test_tensor, target_test_tensor)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "\n",
    "class RotaryPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=1024):\n",
    "        super().__init__()\n",
    "        N = 10000\n",
    "        inv_freq = 1. / (N ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        position = torch.arange(max_seq_len).float()\n",
    "        inv_freq = torch.cat((inv_freq, inv_freq), dim=-1)\n",
    "        sinusoid_inp = torch.outer(position, inv_freq)\n",
    "        self.register_buffer(\"cos\", sinusoid_inp.cos())\n",
    "        self.register_buffer(\"sin\", sinusoid_inp.sin())\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        if seq_len is None:\n",
    "            seq_len = x.size(1)\n",
    "        cos = self.cos[:seq_len].view(1, seq_len, 1, -1)\n",
    "        sin = self.sin[:seq_len].view(1, seq_len, 1, -1)\n",
    "        return apply_rotary_pos_emb(x, cos, sin)\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, hidden_dim, intermediate_dim):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.up = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.down = nn.Linear(intermediate_dim, hidden_dim)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.gate(x)) * self.up(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_heads=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads or num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_groups = num_heads // num_kv_heads\n",
    "        self.dropout = dropout\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, rope=None):\n",
    "        q_batch_size, q_seq_len, hidden_dim = q.shape\n",
    "        k_batch_size, k_seq_len, hidden_dim = k.shape\n",
    "        v_batch_size, v_seq_len, hidden_dim = v.shape\n",
    "\n",
    "        # projection\n",
    "        q = self.q_proj(q).view(q_batch_size, q_seq_len, -1, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(k).view(k_batch_size, k_seq_len, -1, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(v).view(v_batch_size, v_seq_len, -1, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # apply rotary positional encoding\n",
    "        if rope:\n",
    "            q = rope(q)\n",
    "            k = rope(k)\n",
    "\n",
    "        # compute grouped query attention\n",
    "        q = q.contiguous()\n",
    "        k = k.contiguous()\n",
    "        v = v.contiguous()\n",
    "        output = F.scaled_dot_product_attention(q, k, v,\n",
    "                                                attn_mask=mask,\n",
    "                                                dropout_p=self.dropout,\n",
    "                                                enable_gqa=True)\n",
    "        output = output.transpose(1, 2).reshape(q_batch_size, q_seq_len, hidden_dim).contiguous()\n",
    "        output = self.out_proj(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_heads=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n",
    "        self.mlp = SwiGLU(hidden_dim, 4 * hidden_dim)\n",
    "        self.norm1 = nn.RMSNorm(hidden_dim)\n",
    "        self.norm2 = nn.RMSNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask=None, rope=None):\n",
    "        # self-attention sublayer\n",
    "        out = x\n",
    "        out = self.norm1(x)\n",
    "        out = self.self_attn(out, out, out, mask, rope)\n",
    "        x = out + x\n",
    "        # MLP sublayer\n",
    "        out = self.norm2(x)\n",
    "        out = self.mlp(out)\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_kv_heads=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n",
    "        self.cross_attn = GQA(hidden_dim, num_heads, num_kv_heads, dropout)\n",
    "        self.mlp = SwiGLU(hidden_dim, 4 * hidden_dim)\n",
    "        self.norm1 = nn.RMSNorm(hidden_dim)\n",
    "        self.norm2 = nn.RMSNorm(hidden_dim)\n",
    "        self.norm3 = nn.RMSNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, enc_out, mask=None, rope=None):\n",
    "        # self-attention sublayer\n",
    "        out = x\n",
    "        out = self.norm1(out)\n",
    "        out = self.self_attn(out, out, out, mask, rope)\n",
    "        x = out + x\n",
    "        # cross-attention sublayer\n",
    "        out = self.norm2(x)\n",
    "        out = self.cross_attn(out, enc_out, enc_out, None, rope)\n",
    "        x = out + x\n",
    "        # MLP sublayer\n",
    "        x = out + x\n",
    "        out = self.norm3(x)\n",
    "        out = self.mlp(out)\n",
    "        return out + x\n",
    "\n",
    "## The actual model we ahould be able to use\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, num_kv_heads, hidden_dim,\n",
    "                 max_seq_len, vocab_size_src, vocab_size_tgt, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rope = RotaryPositionalEncoding(hidden_dim // num_heads, max_seq_len)\n",
    "        self.src_embedding = nn.Embedding(vocab_size_src, hidden_dim)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size_tgt, hidden_dim)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer(hidden_dim, num_heads, num_kv_heads, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderLayer(hidden_dim, num_heads, num_kv_heads, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size_tgt)\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids, src_mask=None, tgt_mask=None):\n",
    "        # Encoder\n",
    "        x = self.src_embedding(src_ids)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, src_mask, self.rope)\n",
    "        enc_out = x\n",
    "        # Decoder\n",
    "        x = self.tgt_embedding(tgt_ids)\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, tgt_mask, self.rope)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "\n",
    "#Need to tune hyperparameters\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "num_kv_heads = 4\n",
    "hidden_dim = 128\n",
    "max_seq_len = max(max_en_len, max_tl_len)\n",
    "vocab_size_in = len(en_tokenizer.word_index)+1\n",
    "vocab_size_out = len(tl_tokenizer.word_index)+1\n",
    "dropout = .1\n",
    "\n",
    "model = Transformer(num_layers, num_heads, num_kv_heads, hidden_dim, max_seq_len, vocab_size_in, vocab_size_out, dropout) # this is assuming we use the transformer class in the example_model.py script\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "n_epochs = 60\n",
    "lr = .005\n",
    "n_warmup = 1000\n",
    "gradient_clip = 5.0\n",
    "best_loss = float('inf')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "warmup_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=n_warmup)\n",
    "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs * len(train_dataloader) - n_warmup)\n",
    "scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[n_warmup])\n",
    "\n",
    "def create_causal_mask(seq_len, device):\n",
    "    mask = torch.triu(torch.full((seq_len, seq_len), float('-inf'), device=device), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "def create_padding_mask(batch, padding_token_id):\n",
    "    batch_size, seq_len = batch.shape\n",
    "    device = batch.device\n",
    "    padded = torch.zeros_like(batch, device=device).float().masked_fill(batch == padding_token_id, float('-inf'))\n",
    "    mask = torch.zeros(batch_size, seq_len, seq_len, device=device) + padded[:,:,None] + padded[:,None,:]\n",
    "    return mask[:, None, :, :]\n",
    "\n",
    "pad_token_id = 0\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for source_lang, target_lang in tqdm.tqdm(train_dataloader):\n",
    "        source_lang = source_lang.to(device)\n",
    "        target_lang = target_lang.to(device)\n",
    "        src_mask = create_padding_mask(source_lang, pad_token_id)\n",
    "        tgt_mask = create_causal_mask(target_lang.shape[1], device).unsqueeze(0) + create_padding_mask(target_lang, pad_token_id)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(source_lang, target_lang, src_mask, tgt_mask)\n",
    "        loss = loss_fn(outputs[:, :-1, :].reshape(-1, outputs.shape[-1]), target_lang[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        if gradient_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip, error_if_nonfinite=False)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}; Avg loss {epoch_loss/len(train_dataloader)}; Latest loss {loss.item()}\")\n",
    "    train_losses.append(epoch_loss/len(train_dataloader))\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for source_lang, target_lang in tqdm.tqdm(test_dataloader):\n",
    "            source_lang = source_lang.to(device)\n",
    "            target_lang = target_lang.to(device)\n",
    "            src_mask = create_padding_mask(source_lang, pad_token_id)\n",
    "            tgt_mask = create_causal_mask(target_lang.shape[1], device).unsqueeze(0) + create_padding_mask(target_lang, pad_token_id)\n",
    "            outputs = model(source_lang, target_lang, src_mask, tgt_mask)\n",
    "            loss = loss_fn(outputs[:, :-1, :].reshape(-1, outputs.shape[-1]), target_lang[:, 1:].reshape(-1))\n",
    "            epoch_loss += loss.item()\n",
    "    print(f\"Eval loss: {epoch_loss/len(test_dataloader)}\")\n",
    "    test_losses.append(epoch_loss/len(test_dataloader))\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), \"model_out.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ac32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "max_len = max_seq_len\n",
    "samples = random.sample(range(len(test_dataset)), 5)\n",
    "for element in samples:\n",
    "  with torch.no_grad():\n",
    "      start_token = torch.tensor([1]).to(device)\n",
    "      en_ids, true_tl = test_dataset[element]\n",
    "\n",
    "      en_ids = en_ids.unsqueeze(0).to(device)\n",
    "      src_mask = create_padding_mask(en_ids, pad_token_id)\n",
    "      x = model.src_embedding(en_ids)\n",
    "      for encoder in model.encoders:\n",
    "          x = encoder(x, src_mask, model.rope)\n",
    "      enc_out = x\n",
    "      tl_ids = start_token.unsqueeze(0).to(device)\n",
    "      for _ in range(max_len):\n",
    "        tgt_mask = create_causal_mask(tl_ids.shape[0], device).unsqueeze(0) + create_padding_mask(tl_ids, pad_token_id)\n",
    "        x = model.tgt_embedding(tl_ids)\n",
    "        for decoder in model.decoders:\n",
    "          x = decoder(x, enc_out, tgt_mask, model.rope)\n",
    "        outputs = model.out(x)\n",
    "        outputs = outputs.argmax(dim=-1)\n",
    "        tl_ids = torch.cat([tl_ids, outputs[:, -1:]], axis=-1)\n",
    "        if tl_ids[0, -1] == 2:\n",
    "            break\n",
    "\n",
    "          # Decode the predicted IDs\n",
    "  pred_tl = tl_tokenizer.sequences_to_texts([tl_ids[0].tolist()])\n",
    "  print(f\"English: {en_tokenizer.sequences_to_texts([en_ids[0].tolist()])}\")\n",
    "  print(f\"True Tagalog: {tl_tokenizer.sequences_to_texts([true_tl.tolist()])}\")\n",
    "  print(f\"Predicted: {pred_tl}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "truths = []\n",
    "preds = []\n",
    "for element in range(len(test_dataset):\n",
    "  with torch.no_grad():\n",
    "      start_token = torch.tensor([1]).to(device)\n",
    "      en_ids, true_tl = test_dataset[element]\n",
    "\n",
    "      en_ids = en_ids.unsqueeze(0).to(device)\n",
    "      src_mask = create_padding_mask(en_ids, pad_token_id)\n",
    "      x = model.src_embedding(en_ids)\n",
    "      for encoder in model.encoders:\n",
    "          x = encoder(x, src_mask, model.rope)\n",
    "      enc_out = x\n",
    "      tl_ids = start_token.unsqueeze(0).to(device)\n",
    "      for _ in range(max_len):\n",
    "        tgt_mask = create_causal_mask(tl_ids.shape[0], device).unsqueeze(0) + create_padding_mask(tl_ids, pad_token_id)\n",
    "        x = model.tgt_embedding(tl_ids)\n",
    "        for decoder in model.decoders:\n",
    "          x = decoder(x, enc_out, tgt_mask, model.rope)\n",
    "        outputs = model.out(x)\n",
    "        outputs = outputs.argmax(dim=-1)\n",
    "        tl_ids = torch.cat([tl_ids, outputs[:, -1:]], axis=-1)\n",
    "        if tl_ids[0, -1] == 2:\n",
    "            break\n",
    "\n",
    "          # Decode the predicted IDs\n",
    "  pred_tl = tl_tokenizer.sequences_to_texts([tl_ids[0].tolist()])\n",
    "\n",
    "  truths.append(tl_tokenizer.sequences_to_texts([true_tl.tolist()]))\n",
    "  preds.appends(pred_tl)\n",
    "\n",
    "\n",
    "with open('truths.pickle', 'wb') as pkl_file:\n",
    "    pickle.dump(truths, pkl_file)\n",
    "\n",
    "with open('preds.pickle', 'wb') as pkl_file:\n",
    "    pickle.dump(preds, pkl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
